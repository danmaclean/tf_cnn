run_dir,eval_loss,eval_acc,metric_loss,metric_acc,metric_val_loss,metric_val_acc,flag_do_drop1,flag_drop1,flag_do_norm1,flag_do_norm2,flag_batch_size,flag_epochs,samples,validation_samples,batch_size,epochs,epochs_completed,metrics,model,loss_function,optimizer,learning_rate,script,start,end,completed,output,source_code,context,type
runs/2019-03-11T13-11-53Z,0.1582,0.949,0.0611,0.9822,0.1424,0.9512,TRUE,0.01,FALSE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T13-11-53Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:11:53Z,2019-03-11T13:12:21Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1581671

$acc
[1] 0.9490022
",runs/2019-03-11T13-11-53Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-11-02Z,0.1512,0.9501,0.0501,0.9865,0.1476,0.9401,FALSE,0.1,TRUE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T13-11-02Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:11:02Z,2019-03-11T13:11:53Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1512078

$acc
[1] 0.9501109
",runs/2019-03-11T13-11-02Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-10-19Z,0.2453,0.9058,0.2093,0.9151,0.2504,0.8991,FALSE,0.01,TRUE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T13-10-19Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:10:19Z,2019-03-11T13:11:02Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.245289

$acc
[1] 0.905765
",runs/2019-03-11T13-10-19Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-09-57Z,0.2142,0.9157,0.1923,0.9251,0.2147,0.9169,FALSE,0.01,FALSE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T13-09-57Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:09:57Z,2019-03-11T13:10:19Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2142178

$acc
[1] 0.9157428
",runs/2019-03-11T13-09-57Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-09-07Z,0.1785,0.9379,0.0757,0.9723,0.1503,0.9435,TRUE,0.1,TRUE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T13-09-07Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:09:07Z,2019-03-11T13:09:57Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1784939

$acc
[1] 0.9379157
",runs/2019-03-11T13-09-07Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-08-38Z,0.1382,0.9534,0.0775,0.9757,0.1464,0.9512,TRUE,0.1,FALSE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T13-08-38Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:08:38Z,2019-03-11T13:09:07Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.13815

$acc
[1] 0.9534368
",runs/2019-03-11T13-08-38Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-08-16Z,0.1956,0.929,0.13,0.9539,0.1801,0.9335,FALSE,0.1,FALSE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T13-08-16Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:08:16Z,2019-03-11T13:08:38Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1956482

$acc
[1] 0.9290466
",runs/2019-03-11T13-08-16Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-07-33Z,0.1746,0.939,0.154,0.942,0.1903,0.9335,FALSE,0.05,TRUE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T13-07-33Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:07:33Z,2019-03-11T13:08:16Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.174625

$acc
[1] 0.9390244
",runs/2019-03-11T13-07-33Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-06-49Z,0.2413,0.9024,0.1685,0.9359,0.216,0.9002,FALSE,0.05,TRUE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T13-06-49Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:06:49Z,2019-03-11T13:07:32Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2413319

$acc
[1] 0.902439
",runs/2019-03-11T13-06-49Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-06-05Z,0.1749,0.9346,0.1453,0.9462,0.1665,0.9335,TRUE,0.1,TRUE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T13-06-05Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:06:05Z,2019-03-11T13:06:49Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1748678

$acc
[1] 0.9345898
",runs/2019-03-11T13-06-05Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-05-21Z,0.2128,0.9113,0.1901,0.9233,0.2147,0.9202,TRUE,0.01,TRUE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T13-05-21Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:05:21Z,2019-03-11T13:06:05Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2127757

$acc
[1] 0.9113082
",runs/2019-03-11T13-05-21Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-04-31Z,0.189,0.9224,0.0895,0.9729,0.1825,0.9257,FALSE,0.01,TRUE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T13-04-31Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:04:31Z,2019-03-11T13:05:21Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1890445

$acc
[1] 0.9223947
",runs/2019-03-11T13-04-31Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-04-08Z,0.2198,0.9157,0.2143,0.9159,0.2143,0.9191,FALSE,0.05,FALSE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T13-04-08Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:04:09Z,2019-03-11T13:04:31Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2197777

$acc
[1] 0.9157428
",runs/2019-03-11T13-04-08Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-03-18Z,0.1644,0.9435,0.059,0.9828,0.1657,0.9412,TRUE,0.01,TRUE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T13-03-18Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:03:19Z,2019-03-11T13:04:08Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1643551

$acc
[1] 0.943459
",runs/2019-03-11T13-03-18Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-02-35Z,0.1773,0.9313,0.162,0.9357,0.1837,0.9368,TRUE,0.05,TRUE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T13-02-35Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:02:35Z,2019-03-11T13:03:18Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1772551

$acc
[1] 0.9312639
",runs/2019-03-11T13-02-35Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-02-13Z,0.2187,0.9202,0.221,0.9139,0.2013,0.9146,TRUE,0.1,FALSE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T13-02-13Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:02:13Z,2019-03-11T13:02:35Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2187064

$acc
[1] 0.9201774
",runs/2019-03-11T13-02-13Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-01-51Z,0.2267,0.9069,0.1889,0.9233,0.1943,0.9257,FALSE,0.05,FALSE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T13-01-51Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:01:51Z,2019-03-11T13:02:13Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.226665

$acc
[1] 0.9068736
",runs/2019-03-11T13-01-51Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-01-07Z,0.2231,0.9146,0.2114,0.9168,0.199,0.9257,TRUE,0.1,TRUE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T13-01-07Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:01:07Z,2019-03-11T13:01:51Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2230977

$acc
[1] 0.9146341
",runs/2019-03-11T13-01-07Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T13-00-39Z,0.1575,0.9557,0.0665,0.9806,0.1311,0.9568,TRUE,0.01,FALSE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T13-00-39Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T13:00:39Z,2019-03-11T13:01:07Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1574942

$acc
[1] 0.9556541
",runs/2019-03-11T13-00-39Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-59-49Z,0.1309,0.9534,0.0459,0.9881,0.1505,0.9457,FALSE,0.1,TRUE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-59-49Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:59:49Z,2019-03-11T13:00:39Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1308701

$acc
[1] 0.9534368
",runs/2019-03-11T12-59-49Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-59-27Z,0.1849,0.9224,0.1883,0.9275,0.2072,0.9157,TRUE,0.1,FALSE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-59-27Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:59:27Z,2019-03-11T12:59:49Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1848714

$acc
[1] 0.9223947
",runs/2019-03-11T12-59-27Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-58-58Z,0.1376,0.9457,0.0735,0.9751,0.1577,0.9435,TRUE,0.1,FALSE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-58-58Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:58:58Z,2019-03-11T12:59:27Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1376063

$acc
[1] 0.9456763
",runs/2019-03-11T12-58-58Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-58-14Z,0.189,0.9335,0.1501,0.9428,0.1619,0.9435,FALSE,0.01,TRUE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-58-14Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:58:14Z,2019-03-11T12:58:58Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.188994

$acc
[1] 0.9334812
",runs/2019-03-11T12-58-14Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-57-52Z,0.2122,0.9191,0.1987,0.9197,0.2105,0.9124,TRUE,0.1,FALSE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-57-52Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:57:52Z,2019-03-11T12:58:14Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2122498

$acc
[1] 0.9190687
",runs/2019-03-11T12-57-52Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-57-02Z,0.2213,0.9346,0.0638,0.9803,0.1649,0.9368,TRUE,0.1,TRUE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-57-02Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:57:02Z,2019-03-11T12:57:52Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2213314

$acc
[1] 0.9345898
",runs/2019-03-11T12-57-02Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-56-34Z,0.1917,0.9368,0.0616,0.9814,0.1729,0.9424,TRUE,0.05,FALSE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-56-34Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:56:34Z,2019-03-11T12:57:02Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1916556

$acc
[1] 0.9368071
",runs/2019-03-11T12-56-34Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-55-51Z,0.1755,0.9279,0.1492,0.9455,0.1714,0.929,TRUE,0.05,TRUE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-55-51Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:55:51Z,2019-03-11T12:56:34Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.175537

$acc
[1] 0.9279379
",runs/2019-03-11T12-55-51Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-55-22Z,0.2093,0.929,0.0915,0.9677,0.1816,0.939,FALSE,0.05,FALSE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-55-22Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:55:22Z,2019-03-11T12:55:51Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2092525

$acc
[1] 0.9290466
",runs/2019-03-11T12-55-22Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-54-54Z,0.157,0.9512,0.0744,0.9764,0.1597,0.9401,FALSE,0.01,FALSE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-54-54Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:54:54Z,2019-03-11T12:55:22Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1570334

$acc
[1] 0.9512195
",runs/2019-03-11T12-54-54Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-54-10Z,0.2275,0.9213,0.2011,0.9226,0.2386,0.9013,FALSE,0.01,TRUE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-54-10Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:54:10Z,2019-03-11T12:54:54Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2274982

$acc
[1] 0.921286
",runs/2019-03-11T12-54-10Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-53-26Z,0.1969,0.9324,0.1499,0.9442,0.2043,0.9246,TRUE,0.05,TRUE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-53-26Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:53:26Z,2019-03-11T12:54:10Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1968684

$acc
[1] 0.9323725
",runs/2019-03-11T12-53-26Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-52-36Z,0.1676,0.9534,0.0391,0.9904,0.1697,0.9324,FALSE,0.05,TRUE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-52-36Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:52:36Z,2019-03-11T12:53:26Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1675558

$acc
[1] 0.9534368
",runs/2019-03-11T12-52-36Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-51-53Z,0.1658,0.9313,0.1335,0.9513,0.1872,0.9224,FALSE,0.1,TRUE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-51-53Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:51:53Z,2019-03-11T12:52:36Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1658408

$acc
[1] 0.9312639
",runs/2019-03-11T12-51-53Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-51-31Z,0.231,0.9135,0.2053,0.9186,0.2097,0.9157,FALSE,0.1,FALSE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-51-31Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:51:31Z,2019-03-11T12:51:53Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2309998

$acc
[1] 0.9135255
",runs/2019-03-11T12-51-31Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-50-42Z,0.1443,0.9468,0.0695,0.9793,0.1582,0.9479,FALSE,0.01,TRUE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-50-42Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:50:42Z,2019-03-11T12:51:31Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1442894

$acc
[1] 0.9467849
",runs/2019-03-11T12-50-42Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-49-51Z,0.1326,0.9501,0.0702,0.9748,0.1387,0.9457,TRUE,0.1,TRUE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-49-51Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:49:52Z,2019-03-11T12:50:42Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1326244

$acc
[1] 0.9501109
",runs/2019-03-11T12-49-51Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-49-23Z,0.1834,0.939,0.0587,0.9835,0.2246,0.9324,FALSE,0.05,FALSE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-49-23Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:49:23Z,2019-03-11T12:49:51Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1833524

$acc
[1] 0.9390244
",runs/2019-03-11T12-49-23Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-48-33Z,0.188,0.9435,0.0628,0.9791,0.1572,0.9446,TRUE,0.1,TRUE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-48-33Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:48:33Z,2019-03-11T12:49:23Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.187954

$acc
[1] 0.943459
",runs/2019-03-11T12-48-33Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-48-05Z,0.1799,0.9313,0.064,0.9799,0.1814,0.9213,TRUE,0.05,FALSE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-48-05Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:48:05Z,2019-03-11T12:48:33Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1798616

$acc
[1] 0.9312639
",runs/2019-03-11T12-48-05Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-47-22Z,0.1777,0.929,0.1585,0.9374,0.1651,0.9368,FALSE,0.01,TRUE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-47-22Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:47:22Z,2019-03-11T12:48:05Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1776501

$acc
[1] 0.9290466
",runs/2019-03-11T12-47-22Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-46-38Z,0.215,0.9113,0.1598,0.9406,0.2117,0.9157,TRUE,0.05,TRUE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-46-38Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:46:38Z,2019-03-11T12:47:22Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2149903

$acc
[1] 0.9113082
",runs/2019-03-11T12-46-38Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-46-17Z,0.2357,0.9035,0.2195,0.9136,0.2262,0.9024,FALSE,0.05,FALSE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-46-17Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:46:17Z,2019-03-11T12:46:38Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.235651

$acc
[1] 0.9035477
",runs/2019-03-11T12-46-17Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-45-33Z,0.1915,0.9302,0.1613,0.9374,0.1822,0.9313,FALSE,0.1,TRUE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-45-33Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:45:33Z,2019-03-11T12:46:16Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1915035

$acc
[1] 0.9301552
",runs/2019-03-11T12-45-33Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-45-05Z,0.1507,0.9379,0.0615,0.9814,0.1523,0.9401,TRUE,0.05,FALSE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-45-05Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:45:05Z,2019-03-11T12:45:33Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1507092

$acc
[1] 0.9379157
",runs/2019-03-11T12-45-05Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-44-43Z,0.1947,0.9279,0.1585,0.938,0.1669,0.929,TRUE,0.01,FALSE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-44-43Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:44:43Z,2019-03-11T12:45:05Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1947405

$acc
[1] 0.9279379
",runs/2019-03-11T12-44-43Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-44-13Z,0.1621,0.9468,0.0752,0.973,0.145,0.9424,TRUE,0.1,FALSE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-44-13Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:44:13Z,2019-03-11T12:44:43Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1621406

$acc
[1] 0.9467849
",runs/2019-03-11T12-44-13Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-43-29Z,0.183,0.9357,0.1322,0.9532,0.1741,0.9268,FALSE,0.1,TRUE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-43-29Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:43:29Z,2019-03-11T12:44:13Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1829815

$acc
[1] 0.9356984
",runs/2019-03-11T12-43-29Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-42-46Z,0.2004,0.9213,0.1704,0.9386,0.2044,0.9302,TRUE,0.01,TRUE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-42-46Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:42:46Z,2019-03-11T12:43:29Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2003725

$acc
[1] 0.921286
",runs/2019-03-11T12-42-46Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-42-24Z,0.2527,0.8914,0.2432,0.9026,0.2368,0.9035,TRUE,0.05,FALSE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-42-24Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:42:24Z,2019-03-11T12:42:46Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2527198

$acc
[1] 0.8913525
",runs/2019-03-11T12-42-24Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-41-41Z,0.2338,0.9069,0.1676,0.9384,0.2179,0.9169,FALSE,0.05,TRUE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-41-41Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:41:41Z,2019-03-11T12:42:24Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2337886

$acc
[1] 0.9068736
",runs/2019-03-11T12-41-41Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-40-51Z,0.1682,0.939,0.0624,0.9835,0.2149,0.9202,FALSE,0.05,TRUE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-40-51Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:40:51Z,2019-03-11T12:41:41Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1681902

$acc
[1] 0.9390244
",runs/2019-03-11T12-40-51Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-40-02Z,0.2043,0.9357,0.0657,0.9833,0.2095,0.9169,FALSE,0.1,TRUE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-40-02Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:40:02Z,2019-03-11T12:40:51Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2043451

$acc
[1] 0.9356984
",runs/2019-03-11T12-40-02Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-39-13Z,0.1762,0.9379,0.053,0.9893,0.1876,0.929,FALSE,0.05,TRUE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-39-13Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:39:13Z,2019-03-11T12:40:02Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1762201

$acc
[1] 0.9379157
",runs/2019-03-11T12-39-13Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-38-30Z,0.2157,0.918,0.1312,0.9509,0.2608,0.9047,FALSE,0.1,TRUE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-38-30Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:38:30Z,2019-03-11T12:39:13Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2157305

$acc
[1] 0.9179601
",runs/2019-03-11T12-38-30Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-38-08Z,0.1996,0.9191,0.1908,0.92490000000000006,0.1916,0.9246,TRUE,0.05,FALSE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-38-08Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:38:08Z,2019-03-11T12:38:30Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1996452

$acc
[1] 0.9190687
",runs/2019-03-11T12-38-08Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-37-46Z,0.2134,0.9135,0.1718,0.9345,0.1914,0.9213,FALSE,0.05,FALSE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-37-46Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:37:46Z,2019-03-11T12:38:08Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.213361

$acc
[1] 0.9135255
",runs/2019-03-11T12-37-46Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-36-57Z,0.2064,0.9313,0.07,0.981,0.2279,0.9213,FALSE,0.05,TRUE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-36-57Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:36:57Z,2019-03-11T12:37:46Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2063838

$acc
[1] 0.9312639
",runs/2019-03-11T12-36-57Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-36-35Z,0.25,0.9013,0.2551,0.8884,0.2756,0.8836,TRUE,0.01,FALSE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-36-35Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:36:35Z,2019-03-11T12:36:57Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.250034

$acc
[1] 0.9013304
",runs/2019-03-11T12-36-35Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-36-07Z,0.1333,0.949,0.0448,0.9899,0.1386,0.9457,TRUE,0.01,FALSE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-36-07Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:36:07Z,2019-03-11T12:36:35Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1333475

$acc
[1] 0.9490022
",runs/2019-03-11T12-36-07Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-35-45Z,0.2135,0.9202,0.2188,0.9062,0.2173,0.9157,TRUE,0.05,FALSE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-35-45Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:35:45Z,2019-03-11T12:36:07Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2135163

$acc
[1] 0.9201774
",runs/2019-03-11T12-35-45Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-35-17Z,0.142,0.9435,0.0596,0.9839,0.1528,0.9424,FALSE,0.1,FALSE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-35-17Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:35:17Z,2019-03-11T12:35:45Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1420387

$acc
[1] 0.943459
",runs/2019-03-11T12-35-17Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-34-49Z,0.1723,0.9379,0.104,0.9655,0.2086,0.918,FALSE,0.01,FALSE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-34-49Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:34:49Z,2019-03-11T12:35:17Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1722611

$acc
[1] 0.9379157
",runs/2019-03-11T12-34-49Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-34-06Z,0.2072,0.918,0.1996,0.9212,0.1938,0.929,TRUE,0.01,TRUE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-34-06Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:34:06Z,2019-03-11T12:34:49Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2072088

$acc
[1] 0.9179601
",runs/2019-03-11T12-34-06Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-33-33Z,0.2019,0.9246,0.0983,0.9668,0.1937,0.9357,FALSE,0.1,FALSE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-33-33Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:33:33Z,2019-03-11T12:34:06Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2018662

$acc
[1] 0.924612
",runs/2019-03-11T12-33-33Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-32-56Z,0.1849,0.9424,0.0559,0.9888,0.2181,0.9224,FALSE,0.1,FALSE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-32-56Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:32:56Z,2019-03-11T12:33:33Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1849315

$acc
[1] 0.9423503
",runs/2019-03-11T12-32-56Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-32-21Z,0.1487,0.9601,0.0625,0.9816,0.1282,0.9545,FALSE,0.01,FALSE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-32-21Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:32:21Z,2019-03-11T12:32:56Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1487227

$acc
[1] 0.9600887
",runs/2019-03-11T12-32-21Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-31-23Z,0.2271,0.929,0.0711,0.9817,0.2139,0.9368,TRUE,0.01,TRUE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-31-23Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:31:23Z,2019-03-11T12:32:21Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2270731

$acc
[1] 0.9290466
",runs/2019-03-11T12-31-23Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-30-55Z,0.218,0.9202,0.1962,0.9212,0.2023,0.908,TRUE,0.01,FALSE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-30-55Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:30:55Z,2019-03-11T12:31:23Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2180045

$acc
[1] 0.9201774
",runs/2019-03-11T12-30-55Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-30-27Z,0.1812,0.9235,0.1805,0.9333,0.1728,0.9246,TRUE,0.01,FALSE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-30-27Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:30:27Z,2019-03-11T12:30:55Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1811836

$acc
[1] 0.9235033
",runs/2019-03-11T12-30-27Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-29-52Z,0.1728,0.9468,0.0434,0.9899,0.1547,0.949,FALSE,0.01,FALSE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-29-52Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:29:52Z,2019-03-11T12:30:27Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.172843

$acc
[1] 0.9467849
",runs/2019-03-11T12-29-52Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-28-49Z,0.1985,0.9324,0.0584,0.9822,0.1459,0.9412,TRUE,0.05,TRUE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-28-49Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:28:49Z,2019-03-11T12:29:52Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1985087

$acc
[1] 0.9323725
",runs/2019-03-11T12-28-49Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-27-48Z,0.1446,0.9412,0.041,0.9923,0.1473,0.9401,FALSE,0.1,TRUE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-27-48Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:27:48Z,2019-03-11T12:28:49Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1445519

$acc
[1] 0.9412417
",runs/2019-03-11T12-27-48Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-26-50Z,0.1723,0.9424,0.0322,0.9943,0.1786,0.9335,FALSE,0.01,TRUE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-26-50Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:26:50Z,2019-03-11T12:27:48Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1722947

$acc
[1] 0.9423503
",runs/2019-03-11T12-26-50Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-26-22Z,0.2212,0.9124,0.1953,0.9222,0.2195,0.9069,TRUE,0.05,FALSE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-26-22Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:26:22Z,2019-03-11T12:26:50Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2212299

$acc
[1] 0.9124169
",runs/2019-03-11T12-26-22Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-25-47Z,0.1964,0.9357,0.0601,0.9843,0.2108,0.9302,TRUE,0.01,FALSE,TRUE,512,40,6900,902,512,30,30,runs/2019-03-11T12-25-47Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:25:47Z,2019-03-11T12:26:22Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1963948

$acc
[1] 0.9356984
",runs/2019-03-11T12-25-47Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-25-13Z,0.1525,0.9435,0.0656,0.9822,0.1379,0.949,FALSE,0.1,FALSE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-25-13Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:25:13Z,2019-03-11T12:25:47Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1524675

$acc
[1] 0.943459
",runs/2019-03-11T12-25-13Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-24-39Z,0.1509,0.9523,0.0462,0.989,0.1685,0.9424,FALSE,0.05,FALSE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-24-39Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:24:39Z,2019-03-11T12:25:13Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1509358

$acc
[1] 0.9523282
",runs/2019-03-11T12-24-39Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-24-05Z,0.1909,0.9302,0.0675,0.9813,0.1715,0.9401,FALSE,0.05,FALSE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-24-05Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:24:05Z,2019-03-11T12:24:39Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1908735

$acc
[1] 0.9301552
",runs/2019-03-11T12-24-05Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-23-35Z,0.2711,0.8836,0.215,0.9155,0.2198,0.9157,FALSE,0.1,FALSE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-23-35Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:23:35Z,2019-03-11T12:24:05Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2710718

$acc
[1] 0.883592
",runs/2019-03-11T12-23-35Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-23-00Z,0.2416,0.9035,0.2066,0.9181,0.2095,0.9202,TRUE,0.1,FALSE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-23-00Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:23:00Z,2019-03-11T12:23:35Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2416159

$acc
[1] 0.9035477
",runs/2019-03-11T12-23-00Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-22-04Z,0.1694,0.9346,0.0317,0.9941,0.1587,0.9468,TRUE,0.01,TRUE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-22-04Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:22:04Z,2019-03-11T12:23:00Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1693903

$acc
[1] 0.9345898
",runs/2019-03-11T12-22-04Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-21-14Z,0.1692,0.9435,0.1555,0.9406,0.1752,0.9412,TRUE,0.01,TRUE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-21-14Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:21:14Z,2019-03-11T12:22:04Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1692035

$acc
[1] 0.943459
",runs/2019-03-11T12-21-14Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-20-25Z,0.2263,0.918,0.2364,0.9083,0.1952,0.9302,TRUE,0.1,TRUE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-20-25Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:20:25Z,2019-03-11T12:21:14Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2262969

$acc
[1] 0.9179601
",runs/2019-03-11T12-20-25Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-19-36Z,0.1873,0.9335,0.1562,0.9384,0.1933,0.9246,TRUE,0.1,TRUE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-19-36Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:19:36Z,2019-03-11T12:20:25Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1873047

$acc
[1] 0.9334812
",runs/2019-03-11T12-19-36Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-18-41Z,0.1524,0.9457,0.0614,0.9832,0.1593,0.9324,TRUE,0.01,TRUE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-18-41Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:18:41Z,2019-03-11T12:19:36Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1524103

$acc
[1] 0.9456763
",runs/2019-03-11T12-18-41Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-17-47Z,0.1497,0.9468,0.0394,0.9916,0.1783,0.9379,FALSE,0.01,TRUE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-17-47Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:17:47Z,2019-03-11T12:18:41Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1496618

$acc
[1] 0.9467849
",runs/2019-03-11T12-17-47Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-17-19Z,0.3304,0.8525,0.3441,0.8413,0.3361,0.8459,FALSE,0.01,FALSE,FALSE,512,40,6900,902,512,30,30,runs/2019-03-11T12-17-19Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:17:19Z,2019-03-11T12:17:47Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.3304098

$acc
[1] 0.8525499
",runs/2019-03-11T12-17-19Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-16-22Z,0.14,0.9424,0.0552,0.9819,0.1715,0.9357,TRUE,0.05,TRUE,TRUE,256,40,6900,902,512,30,30,runs/2019-03-11T12-16-22Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:16:22Z,2019-03-11T12:17:19Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1399796

$acc
[1] 0.9423503
",runs/2019-03-11T12-16-22Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-15-26Z,0.1296,0.9612,0.0493,0.9845,0.1147,0.9534,TRUE,0.05,TRUE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-15-26Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:15:26Z,2019-03-11T12:16:22Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1296347

$acc
[1] 0.9611973
",runs/2019-03-11T12-15-26Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-14-38Z,0.2033,0.9157,0.1834,0.9284,0.2219,0.9058,FALSE,0.05,TRUE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-14-38Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,555
Trainable params: 7,547
Non-trainable params: 8
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:14:38Z,2019-03-11T12:15:26Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.20329

$acc
[1] 0.9157428
",runs/2019-03-11T12-14-38Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-14-11Z,0.244,0.9124,0.2183,0.9075,0.2278,0.9002,FALSE,0.1,FALSE,FALSE,256,40,6900,902,512,30,30,runs/2019-03-11T12-14-11Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:14:11Z,2019-03-11T12:14:38Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2440481

$acc
[1] 0.9124169
",runs/2019-03-11T12-14-11Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-13-41Z,0.2453,0.9113,0.2281,0.9059,0.2113,0.9202,FALSE,0.01,FALSE,FALSE,512,30,6900,902,512,30,30,runs/2019-03-11T12-13-41Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:13:42Z,2019-03-11T12:14:11Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2452531

$acc
[1] 0.9113082
",runs/2019-03-11T12-13-41Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-13-16Z,0.2138,0.9257,0.1684,0.9333,0.1847,0.9324,FALSE,0.01,FALSE,FALSE,256,30,6900,902,512,30,30,runs/2019-03-11T12-13-16Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,539
Trainable params: 7,539
Non-trainable params: 0
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:13:16Z,2019-03-11T12:13:41Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.2137739

$acc
[1] 0.9257206
",runs/2019-03-11T12-13-16Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-12-22Z,0.1807,0.929,0.0633,0.9814,0.1839,0.9302,TRUE,0.05,TRUE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-12-22Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 2, 232, 4)               16          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_2 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,587
Trainable params: 7,563
Non-trainable params: 24
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:12:22Z,2019-03-11T12:13:16Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1806741

$acc
[1] 0.9290466
",runs/2019-03-11T12-12-22Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-11-52Z,0.1489,0.9468,0.1,0.9681,0.1408,0.9501,TRUE,0.05,FALSE,TRUE,256,30,6900,902,512,30,30,runs/2019-03-11T12-11-52Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:11:52Z,2019-03-11T12:12:22Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.148892

$acc
[1] 0.9467849
",runs/2019-03-11T12-11-52Z/tfruns.d/source.tar.gz,local,training
runs/2019-03-11T12-11-20Z,0.1604,0.9324,0.0939,0.9642,0.178,0.9346,TRUE,0.1,FALSE,TRUE,512,30,6900,902,512,30,30,runs/2019-03-11T12-11-20Z/tfruns.d/metrics.json,"Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
separable_conv2d_1 (SeparableConv2D (None, 2, 232, 4)               18          
________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)      (None, 1, 116, 4)               0           
________________________________________________________________________________
separable_conv2d_2 (SeparableConv2D (None, 1, 116, 8)               80          
________________________________________________________________________________
batch_normalization_1 (BatchNormali (None, 1, 116, 8)               32          
________________________________________________________________________________
flatten_1 (Flatten)                 (None, 928)                     0           
________________________________________________________________________________
dense_1 (Dense)                     (None, 8)                       7432        
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 8)                       0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 1)                       9           
================================================================================
Total params: 7,571
Trainable params: 7,555
Non-trainable params: 16
________________________________________________________________________________

",binary_crossentropy,<keras.optimizers.RMSprop>,0.00100000004749745,tunable_model.R,2019-03-11T12:11:20Z,2019-03-11T12:11:52Z,TRUE,"
> library(keras)

> library(magrittr)

> FLAGS <- flags(flag_boolean(""do_drop1"", TRUE, ""do layer1dropout""), 
+     flag_numeric(""drop1"", 0.01, ""drop 1 rate""), flag_boolean(""do_norm1"", 
+    .... [TRUNCATED] 

> final_model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 4, 
+     kernel_size = c(2, 5), activation = ""relu"", input_shape = c( .... [TRUNCATED] 

> if (FLAGS$do_norm1) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_max_pooling_2d(pool_size = c(2, 
+     2))

> final_model %>% layer_separable_conv_2d(filters = 8, 
+     kernel_size = c(2, 5), activation = ""relu"", padding = ""same"")

> if (FLAGS$do_norm2) {
+     final_model %>% layer_batch_normalization()
+ }

> final_model %>% layer_flatten()

> final_model %>% layer_dense(units = 8, activation = ""relu"")

> if (FLAGS$do_drop1) {
+     final_model %>% layer_dropout(FLAGS$drop1)
+ }

> final_model %>% layer_dense(units = 1, activation = ""sigmoid"")

> final_model %>% compile(optimizer = ""rmsprop"", loss = ""binary_crossentropy"", 
+     metrics = c(""accuracy""))

> final_model %>% fit(x_train, y_train, epochs = 30, 
+     batch_size = 512, validation_data = list(x_val, y_val))

> final_model %>% evaluate(x_test, y_test)
$loss
[1] 0.1604218

$acc
[1] 0.9323725
",runs/2019-03-11T12-11-20Z/tfruns.d/source.tar.gz,local,training
