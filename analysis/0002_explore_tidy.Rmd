---
title: "0002_explore_tidy"
author: "Dan MacLean"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(magrittr)
library(ggplot2)
library(ggridges)
library(dplyr)
```

## Explore Model Structures

Further to document `0001_explore` which sets up a preliminary dense network and tries to use a genetic algorithm to find the best layer and neuron hyperparameters. I want to explore non-rectangular structures for the dense network. This is fiddly with GAs, so I'll take a shotgun approach to finding different structures. I'll generate a table of possible structures and randomly try those to find some best ones.

Again the basic model will be the dense layer with a final flattening step. This time layers can have different numbers of neurons in the same model 

```{r, generate_random_structures}

random_structure <- function(max_layers = 16, max_nodes = 256){
  layer_sizes <- 2 ^ (1:8) #layer size will be a power of 2
  layer_sizes <- layer_sizes[layer_sizes < max_nodes]
  layers <- sample(2:max_layers, 1)
  
  list( layers = layers,
        nodes_per_layer = sort(sample(layer_sizes, layers, replace = TRUE), decreasing = TRUE) #minimum layer size is 2, always bigger to smaller
  )
}

make_model <- function(structure = list( layers=3, nodes_per_layer = c(16,16,32) ) ){
  
  
  mod <- keras_model_sequential() %>%
    layer_dense(units = structure$nodes_per_layer[1], activation = "relu", input_shape = c(2, 232))
  
  for (l in 2:structure$layers){
    mod <-mod  %>%
      layer_dense(units = structure$nodes_per_layer[l] )
  }
  
  mod <-mod %>%
    layer_flatten() %>%
    layer_dense(units = 1, activation = "sigmoid")
  
  mod %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
  
  mod
}

# given a model and training and valiidation data returns the accuracy for a run
evaluate_model <- function(mod, x_train = NULL, y_train = NULL, 
                           x_val = NULL, y_val = NULL, 
                           epochs = 20, batch_size = 512, 
                           verbose = 0, full_result = FALSE){
  mod %>% fit(
    x_train,
    y_train,
    epochs = epochs,
    batch_size = batch_size,
    verbose = verbose
  )
  
  result <- mod %>% evaluate(
    x_val,
    y_val,
    verbose = verbose
  )
  if (full_result){
    return(result)
  }
  else {
   return(result$acc) 
  }
}

#given a model structure (x[1] = number of layersl x[2] = number of nodes per layer)
# builds, evaluates and returns accuracy of a model with requested structure, implementing
#Â 4-fold x-validation
model_accuracy <- function(structure, 
                           training_validation_x, training_validation_y, #does not include test data, only that to be k-foldes
                           epochs = 20, batch_size = 512, 
                          verbose = 0){
  
   accuracy <- c()
   #start k-fold here
   k <- 4 
   indices <- sample(1:nrow(training_validation_x))
   folds <- cut(indices, breaks = k, labels = FALSE)
   for (i in 1:k){
     
     val_ind <- which(folds == i, arr.ind = TRUE)
     x_val <- training_validation_x[val_ind,,]
     y_val <- training_validation_y[val_ind]
     x_train <- training_validation_x[-val_ind,,]
     y_train <-training_validation_y[-val_ind]

     m <- make_model(structure = structure )
     a <- evaluate_model(m, 
                x_train = x_train, y_train = y_train, 
                x_val = x_val, y_val = y_val,
                epochs = epochs, batch_size = batch_size, 
                verbose = verbose
                ) 
     accuracy <- c(accuracy, a)
   }
   #end x-fold
  mean(accuracy)
}
```

```{r, run_structures, message=FALSE}
run_random_sweep <- function(number = 100, outfile = "random_structs.RDS"){
  
  arab_data <- readRDS("../data/arab_TF.RDS")
  new_order <- sample(1:8704)
  shuffled_data <- list(
    x = arab_data$x[new_order,,],
    y = as.numeric(arab_data$y[new_order])
  )
  
  
  result <- data.frame(model_id = integer(), 
                       layer_id = integer(), 
                       neurons_in_layer = integer(), 
                       layers = integer(),  
                       accuracy = double()
  )
  
  for(model_id in 1:number){
    message(" running sample ", number)
    structure <- random_structure(max_layers = 12, max_nodes = 64)
    message(" trying layers: ", structure$layers, " :nodes per layer", structure$nodes_per_layer)
    accuracy <- model_accuracy(structure,
                               training_validation_x = shuffled_data$x[1:7802,,],
                               training_validation_y = shuffled_data$y[1:7802]
    ) 
    r <- data.frame( 
      model_id = rep(number, structure$layers), 
      layer_id = 1:structure$layers, 
      neurons_in_layer = structure$nodes_per_layer, 
      layers = rep(structure$layers, structure$layers),
      accuracy = rep(accuracy, structure$layers)
    )
    
    result <- dplyr::bind_rows(result, r)
  }
  
  saveRDS(result, outfile)
}

if ( ! file.exists("random_structs.RDS") ){
  run_random_sweep()
}
```

## How much parameter space did the scan cover?

There were only 100 models tested. Let's see how many combinations of the layer and nodes per layer we covered.

```{r check_hits}
df <- readRDS("random_structs.RDS")

df %>% 
  group_by(layers, neurons_in_layer) %>%
  summarize(
    layer = first(layers),
    neurons = first(neurons_in_layer),
    count = n_distinct(model_id)
    
  ) %>%
ggplot() + aes(as.factor(layer), as.factor(neurons)) + geom_tile(aes(fill = count))
```

So, we got reasonable coverage, multiple models hit large combinations of neurons and layers per neuron, though there is a problem. I was expecting the maximum neurons_per_layer to be 64, but its 32. From misuse of `>` over `>=` in the `random_structure` function. 

## How good were the models?

```{r, examine_sweep}
ggplot(df) + aes(accuracy) + geom_histogram()

summary(df$accuracy)
```

Not many are very much better than the initial quickly chosen one! Let's see how it breaks down by the different variables

```{r, examine_layers}
df %>% 
  group_by(model_id) %>%
  summarize(
    layers = first(layers),
    accuracy = first(accuracy)
  ) %>%
  ggplot() + aes(x = accuracy, y = factor(layers) ) + geom_density_ridges()
```

```{r, count_n_layers_tried}
df %>%
  group_by(layers) %>%
  summarize( 
    times_tried = n_distinct(model_id)
  )
```

Ok, so the best is 12 layers, but it is bi-modal. 12 layers were only tried 6 times, so that second 'best' lump looks like a $ \frac{2 6}$ split. 

Let's try inspecting that 12 layer model.

```{r, 12_layer_inspect, fig.width=13}

df %>% 
  filter(layers == 12) %>%
  ggplot() + aes(x = neurons_in_layer, y = accuracy) + geom_point(aes(colour = factor(model_id))) + facet_grid(~ layer_id)
```

Looks like those two best models are 1 and 60. There isn't anything massively different to the other models here, tendency to be a bit more middling, I.E keep more mid-valued number of neurons earlier on, around layer 5 or so. All the models have low numbers of neurons at the end - a side-effect of sorting the layers. I had expected to see more variability but nope! 

The work is probably being done up to the 6th or so layer and the many 2 neuron layers after probably aren't contributing much. I'll re run on all the data and validate the model 1 structure (the absolute best), and also to briefly test my idea about the layers after 6 a truncated version and a more bespoke one.

So three new model designs to test:

1. The `model 1` structure

```{r, extract_model_1, echo = FALSE}
df %>% filter(model_id == 1) %>% select(layer_id, neurons_in_layer)
```

2. The truncated `model 1` structure

```{r, extract_truncated_model_1, echo = FALSE}
df %>% filter(model_id == 1) %>% select(layer_id, neurons_in_layer) %>% head()
```

3. A similar structure, slightly modified
```{r, made_up_structure, echo = FALSE}
data.frame(layer_id = 1:6, neurons_in_layer = c(32,32,16,16,8,8))
```


```{r, build_models}
mod1_struct = list( layers = 12, nodes_per_layer = c(32,32,16,8,8,8,4,4,4,4,2,2))
mod2_struct = list( layers = 6, nodes_per_layer = c(32,32,16,8,8,8))
mod3_struct = list( layers = 6, nodes_per_layer = c(32,32,16,16,8,8))

model1 <- make_model(structure = mod1_struct )
model2 <- make_model(structure = mod2_struct )
model3 <- make_model(structure = mod3_struct )

arab_data <- readRDS("../data/arab_TF.RDS")
new_order <- sample(1:8704)
shuffled_data <- list(
  x = arab_data$x[new_order,,],
  y = as.numeric(arab_data$y[new_order])
)

train_i <- 1:6900
val_i <- 6901:7802
test_i <- 7803:8704

x_train <- shuffled_data$x[train_i,,]
x_val <- shuffled_data$x[val_i,,]
x_test <- shuffled_data$x[test_i,,]

y_train <- shuffled_data$y[train_i]
y_val <- shuffled_data$y[val_i]
y_test <- shuffled_data$y[test_i]


history1 <- model1 %>% fit(
  x_train,
  y_train,
  epochs = 20,
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

plot(history1)


history2 <- model2 %>% fit(
  x_train,
  y_train,
  epochs = 20,
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

plot(history2)


history3 <- model3 %>% fit(
  x_train,
  y_train,
  epochs = 20,
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

plot(history3)

```

The three models are pretty equal. I'll stick with the general structure of the first model and try to optimise that.