---
title: "0006_convnet_one_channel"
author: "Dan MacLean"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(keras)
library(magrittr)
```

## A one channel convnet

Following on from the convent in `0005_convnet_explore` which uses inputs of 232,232,2 I'll try the original 2, 232,1 data, see if that runs quicker. I think I may be thinking of these things as images too literally. So, loading in the scaled, 2,232,1 shaped data

```{r, load_data}
arab_data <- readRDS("../data/scaled_arab_TF.RDS")
new_order <- sample(1:8704)
shuffled_data <- list(
  x = arab_data$x[new_order,,],
  y = as.numeric(arab_data$y[new_order])
)

train_i <- 1:6900
val_i <- 6901:7802
test_i <- 7803:8704

x_train <- shuffled_data$x[train_i,,]
x_val <- shuffled_data$x[val_i,,]
x_test <- shuffled_data$x[test_i,,]

y_train <- shuffled_data$y[train_i]
y_val <- shuffled_data$y[val_i]
y_test <- shuffled_data$y[test_i]
```

Let's use the rough convnet from the first section of `0005_convnet_explore`. 


```{r one_chan_convnet}


model <- tryCatch({
  keras_model_sequential() %>%
    layer_conv_2d(filters = 32, kernel_size = c(2,5), activation = "relu", input_shape = c(2,232,1), padding = "same") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(2,5), activation = "relu", padding = "same") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%  
    layer_conv_2d(filters = 64, kernel_size = c(2,5), activation = "relu",  padding = "same") %>%
    layer_flatten() %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1, activation = "sigmoid")
},
warning = function(w) {
  print(paste("Warning:", w))
}, 
error = function(e){
  print(paste("Error:", e))
}

)
```

Looks like this error might be related to the number of layers and the max pooling. Let's strip the model back and see


```{r, very_mininal}
model <- tryCatch({
  keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,5), activation = "relu", input_shape = c(2,232,1), padding = "same") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
},
warning = function(w) {
  print(paste("Warning:", w))
}, 
error = function(e){
  print(paste("Error:", e))
}

)

summary(model)
```

Great, so this one builds. Let's see how we can add more layers.. 

```{r}
model <- tryCatch({
  keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,5), activation = "relu", input_shape = c(2,232,1), padding = "same") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,5), activation = "relu", padding = "same") %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
},
warning = function(w) {
  print(paste("Warning:", w))
}, 
error = function(e){
  print(paste("Error:", e))
}

)
summary(model)
```

Right, so this is the biggest model I can seem to get without breaking. Let's give it a whirl.

```{r, test_model}
tryCatch({
  model %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
  
  history <- model %>% fit(
    x_train,
    y_train,
    epochs = 20,
    batch_size = 512, 
    validation_data = list(x_val, y_val)
  )

plot(history)
},
error = function(e){
  print(paste("error: ", e))
}
)
```

Model dies! 

Looks like the input isn't as expected, it looks like it wants a 4D tensor for an image. Not, my2D ones. I'll rework it, manually in the build script, it can be built using

```{bash eval = FALSE}
rake data/scaled_arab_TF_one_channel.RDS
```

Let's load this new data and try it.

```{r, one_channel_data}
arab_data <- readRDS("../data/scaled_arab_TF_one_channel.RDS")
new_order <- sample(1:8704)

shuffled_data <- list(
  x = arab_data$x[new_order,,,, drop = F],
  y = as.numeric(arab_data$y[new_order])
)


train_i <- 1:6900
val_i <- 6901:7802
test_i <- 7803:8704

x_train <- shuffled_data$x[train_i,,, ,drop = F]
x_val <- shuffled_data$x[val_i,,, ,drop = F]
x_test <- shuffled_data$x[test_i,,, ,drop = F]

y_train <- shuffled_data$y[train_i]
y_val <- shuffled_data$y[val_i]
y_test <- shuffled_data$y[test_i]

tryCatch({
  model %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
  
  history <- model %>% fit(
    x_train,
    y_train,
    epochs = 20,
    batch_size = 512, 
    validation_data = list(x_val, y_val)
  )

plot(history)
},
error = function(e){
  print(paste("error: ", e))
}
)

```

Win! What looks like a great, and fast running classification on the whole dataset straight off! It is overfitting after 5 epochs, but this is a great start. We can start to deal with overfitting and then look at feature extraction. 