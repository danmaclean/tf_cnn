{
  "message": "The following flags were provided but not declared: dropout1, filters1, filters2, units1, activation1, activation2, activation3, loss, optimizer",
  "traceback": ["stop(\"The following flags were provided but not declared: \", \n    paste(undeclared, collapse = \", \"), call. = FALSE)", "parse_flags(FLAGS, config, file, arguments)", "flags(flag_string(\"regularizer1\", \"regularizer_l1\", \"regularizer in first convolution layer\"), \n    flag_numeric(\"reg1_rate\", 0.01, \"rate for regularizer1\"), \n    flag_integer(\"filter1\", 4, \"Number of filters for first conv  set\"), \n    flag_string(\"conv_activation1\", \"relu\", \"Activation Function for the first convolutions\"), \n    flag_integer(\"kernel1\", 5, \"Size of kernel in first conv layer\"), \n    flag_boolean(\"do_max_pool1\", TRUE, \"do the max pool between 1st and subsequent conv layers\"), \n    flag_string(\"pool1_type\", \"max_pool\", \"use max or mean pooling\"), \n    flag_integer(\"pool1\", 2, \"Size of first pooling\"), flag_boolean(\"do_drop1\", \n        TRUE, \"use dropout after first conv layer\"), flag_numeric(\"drop1\", \n        0.01, \"Dropout for the pooling\"), flag_boolean(\"do_norm1\", \n        TRUE, \"use batch norm after first conv layer\"), flag_numeric(\"nconvlayers\", \n        1, \"Number of extra convolution layers\"), flag_string(\"regularizer2\", \n        \"regularizer_l1\", \"regularizer in extra convolution layers\"), \n    flag_numeric(\"reg2_rate\", 0.01, \"rate for regularizer2\"), \n    flag_integer(\"filter2\", 8, \"Number of filters for second conv  set\"), \n    flag_string(\"conv_activation2\", \"relu\", \"Activation Function for the first convolutions\"), \n    flag_integer(\"kernel2\", 5, \"Size of kernel in extra conv layers\"), \n    flag_boolean(\"do_max_pool2\", TRUE, \"do the max pool between optional conv layers\"), \n    flag_string(\"pool2_type\", \"max_pool\", \"use max or mean pooling\"), \n    flag_integer(\"pool2\", 2, \"Size of extra conv layer poolings\"), \n    flag_boolean(\"do_drop2\", TRUE, \"use dropout after optional conv layers\"), \n    flag_numeric(\"drop2\", 0.01, \"Dropout for the pooling\"), flag_boolean(\"conv_norm\", \n        TRUE, \"use batch norm after each extra conv layer\"), \n    flag_boolean(\"do_post_conv_norm\", TRUE, \"use batch norm after last conv layer\"), \n    flag_string(\"regularizer_dense1\", \"regularizer_l1\", \"regularizer in first dense layer\"), \n    flag_numeric(\"regdense1_rate\", 0.001, \"regularizer rate in first dense layer\"), \n    flag_numeric(\"ndense\", 1, \"Number of extra dense layers\"), \n    flag_string(\"dense_activation\", \"relu\", \"Activation for the dense layers\"), \n    flag_numeric(\"hidden1\", 8, \"Size of first dense layer\"), \n    flag_boolean(\"do_drop_dense1\", TRUE, \"Dropout for dense layers\"), \n    flag_boolean(\"do_drop_dense2\", TRUE, \"Dropout after dense layers\"), \n    flag_numeric(\"hidden2\", 8, \"Size of subsequent dense layers\"), \n    flag_integer(\"epochs\", 40, \"Number of epochs\"), flag_integer(\"batch_size\", \n        512, \"Batch size\"), flag_string(\"optimiser\", \"rmsprop\", \n        \"Optimiser function\"), flag_numeric(\"learn_rate\", 0.001, \n        \"Learning Rate\"))", "eval(ei, envir)", "eval(ei, envir)", "withVisible(eval(ei, envir))", "source(file = file, local = envir, echo = echo, encoding = encoding)", "withCallingHandlers({\n    source(file = file, local = envir, echo = echo, encoding = encoding)\n    write_run_property(\"completed\", TRUE)\n}, error = function(e) {\n    write_run_metadata(\"error\", list(message = e$message, traceback = capture_stacktrace(sys.calls())))\n    stop(e)\n})", "force(expr)", "with_changed_file_copy(getwd(), run_dir, {\n    write_run_property(\"script\", basename(file))\n    write_run_property(\"start\", as.double(Sys.time()))\n    on.exit(write_run_property(\"end\", as.double(Sys.time())), \n        add = TRUE)\n    on.exit(clear_run(), add = TRUE)\n    on.exit(reset_tf_graph(), add = TRUE)\n    old_width <- getOption(\"width\")\n    options(width = min(100, old_width))\n    on.exit(options(width = old_width), add = TRUE)\n    properties_dir <- file.path(meta_dir(run_dir), \"properties\")\n    output_file <- file(file.path(properties_dir, \"output\"), \n        open = \"wt\", encoding = \"UTF-8\")\n    sink(file = output_file, type = \"output\", split = TRUE)\n    on.exit({\n        sink(type = \"output\")\n        close(output_file)\n    }, add = TRUE)\n    plots_dir <- file.path(run_dir, \"plots\")\n    if (!utils::file_test(\"-d\", plots_dir)) \n        dir.create(plots_dir, recursive = TRUE)\n    png_args <- list(filename = file.path(plots_dir, \"Rplot%03d.png\"), \n        width = 1200, height = 715, res = 192)\n    if (is_windows() && capabilities(\"cairo\")) \n        png_args$type <- \"cairo\"\n    do.call(grDevices::png, png_args)\n    dev_number <- grDevices::dev.cur()\n    on.exit(grDevices::dev.off(dev_number), add = TRUE)\n    message(\"Using run directory \", run_dir)\n    write_run_property(\"completed\", FALSE)\n    withCallingHandlers({\n        source(file = file, local = envir, echo = echo, encoding = encoding)\n        write_run_property(\"completed\", TRUE)\n    }, error = function(e) {\n        write_run_metadata(\"error\", list(message = e$message, \n            traceback = capture_stacktrace(sys.calls())))\n        stop(e)\n    })\n})", "do_training_run(file, run_dir, echo = echo, envir = envir, encoding = encoding)", "training_run(file = file, config = config, flags = flags, properties = properties, \n    run_dir = NULL, echo = echo, view = FALSE, envir = new.env(parent = envir), \n    encoding = encoding)", "tuning_run(\"../scripts/tunable_model.R\", sample = 0.1, echo = FALSE, \n    flags = list(dropout1 = c(0.01, 0.05), filters1 = c(8, 4), \n        filters2 = c(8, 4), kernel1 = c(5, 7, 10), kernel2 = c(5, \n            7, 10), units1 = c(8, 4), activation1 = c(\"relu\", \n            \"sigmoid\", \"softmax\"), activation2 = c(\"relu\", \"sigmoid\", \n            \"softmax\"), activation3 = c(\"relu\", \"sigmoid\", \"softmax\"), \n        epochs = c(30, 40), batch_size = c(256, 512), loss = c(\"binary_crossentropy\"), \n        optimizer = c(\"adam\", \"rmsprop\")))", "eval(expr, envir, enclos)", "eval(expr, envir, enclos)", "withVisible(eval(expr, envir, enclos))", "withCallingHandlers(withVisible(eval(expr, envir, enclos)), warning = wHandler, \n    error = eHandler, message = mHandler)", "handle(ev <- withCallingHandlers(withVisible(eval(expr, envir, \n    enclos)), warning = wHandler, error = eHandler, message = mHandler))", "timing_fn(handle(ev <- withCallingHandlers(withVisible(eval(expr, \n    envir, enclos)), warning = wHandler, error = eHandler, message = mHandler)))", "evaluate_call(expr, parsed$src[[i]], envir = envir, enclos = enclos, \n    debug = debug, last = i == length(out), use_try = stop_on_error != \n        2L, keep_warning = keep_warning, keep_message = keep_message, \n    output_handler = output_handler, include_timing = include_timing)", "evaluate::evaluate(...)", "evaluate(code, envir = env, new_device = FALSE, keep_warning = !isFALSE(options$warning), \n    keep_message = !isFALSE(options$message), stop_on_error = if (options$error && \n        options$include) 0L else 2L, output_handler = knit_handlers(options$render, \n        options))", "in_dir(input_dir(), evaluate(code, envir = env, new_device = FALSE, \n    keep_warning = !isFALSE(options$warning), keep_message = !isFALSE(options$message), \n    stop_on_error = if (options$error && options$include) 0L else 2L, \n    output_handler = knit_handlers(options$render, options)))", "block_exec(params)", "rmarkdown::render(\"/Users/macleand/Desktop/tfs/analysis/0015_tuning_runs.Rmd\", \n    encoding = \"UTF-8\")"]
}
