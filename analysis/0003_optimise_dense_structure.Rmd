---
title: "0003_optimise_dense_structure"
author: "Dan MacLean"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(magrittr)
library(ggplot2)
library(ggridges)
library(dplyr)
```

## Optimise Model Structures

Following the document `0002_explore_tidy` I have a reasonable model structure that I wish to optimise.
The structure has 12 layers (plus an iniital and a flattening layer) of neuron depth `r c(32,32,16,8,8,8,4,4,4,4,2,2)`. 

To optimise I'll test droput and regularization effects on this structure.

```{r, regularisation}


make_model <- function(structure = list( layers=3, nodes_per_layer = c(16,16,32) ), 
                       with_dropout = TRUE, 
                       dropout_rate = 0.5,
                       with_l1 = FALSE,
                       l1_rate = 0.001,
                       with_l2 = FALSE,
                       l2_rate = 0.001
                       ){
  
  
  mod <- keras_model_sequential() %>%
    layer_dense(units = structure$nodes_per_layer[1], activation = "relu", input_shape = c(2, 232))
  
  for (l in 2:structure$layers){
    if (with_l1 ){
      mod <-mod  %>%
        layer_dense(units = structure$nodes_per_layer[l], kernel_regularizer = regularizer_l1(l1_rate) )
    }
    else if (with_l2){
      mod <-mod  %>%
        layer_dense(units = structure$nodes_per_layer[l], kernel_regularizer = regularizer_l2(l2_rate) )
    }
    else{
      mod <-mod  %>%
        layer_dense(units = structure$nodes_per_layer[l] )
    }
  
     if (with_dropout & (l < (structure$layers - 1))) { #no dropout after last layer 
        mod <- mod %>% layer_dropout(rate = dropout_rate)
     }
  }
  
  mod <-mod %>%
    layer_flatten() %>%
    layer_dense(units = 1, activation = "sigmoid")
  
  mod %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
  mod
}

arab_data <- readRDS("../data/arab_TF.RDS")
new_order <- sample(1:8704)
shuffled_data <- list(
  x = arab_data$x[new_order,,],
  y = as.numeric(arab_data$y[new_order])
)

train_i <- 1:6900
val_i <- 6901:7802
test_i <- 7803:8704

x_train <- shuffled_data$x[train_i,,]
x_val <- shuffled_data$x[val_i,,]
x_test <- shuffled_data$x[test_i,,]

y_train <- shuffled_data$y[train_i]
y_val <- shuffled_data$y[val_i]
y_test <- shuffled_data$y[test_i]

mod1_struct = list( layers = 12, nodes_per_layer = c(32,32,16,8,8,8,4,4,4,4,2,2))
model1 <- make_model(structure = mod1_struct, with_dropout = TRUE, dropout_rate = 0.1 )

history1 <- model1 %>% fit(
  x_train,
  y_train,
  epochs = 20,
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

plot(history1)
```

Looks worse! But I'll try longer training times as it seems to still be increasing.

```{r, longer_train}

history2 <- model1 %>% fit(
  x_train,
  y_train,
  epochs = 200,
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)

plot(history2)

```

The dropout improves things if you give it more time. Its still improving but slowly. Let's try a range of dropout rates over 300 epochs.

```{r, dropouts, eval = TRUE, message = FALSE, warning = FALSE, fig.width = 13, cache = TRUE}
result <- data.frame(dropout = double(), epoch = integer(), value = double(), metric = character(), data = character() )

for (dr in c(0.001, 0.01, 0.05, 0.1, 0.2 )){
  epochs <- 300
  m <- make_model(structure = mod1_struct, with_dropout = TRUE, dropout_rate = dr )
  h <- m %>% fit(
      x_train,
      y_train,
      epochs = epochs,
      batch_size = 512, 
      validation_data = list(x_val, y_val)
  )
  df_h <- as.data.frame( h )
  df_h$dropout <- rep(dr, epochs) 
  result <- dplyr::bind_rows(result, df_h)
  
}

ggplot(result) + aes(epoch, value) + geom_point(aes(colour = data), alpha= 0.2) + geom_smooth(aes(colour = data)) + facet_grid(metric ~ dropout, scales = "free")
```

This works really well with a slight dropout. 0.001 is getting us to 0.85 accuracy and over. There isn't much improvement over 0.01, so it probably doesn't make much sense to go lower. The thing is still improving, though, but adding epochs is likey to overfit, so I shall resist the temptation to do more. 

## Weight Regularization

Adding slight weight regularization can reduce overfitting, hopefully help us move into the 90% accuracy bracket. Let's try the same model with the best dropout and an l1 rate of 0.01

```{r, l1_regularizer, cache = TRUE}
m <- make_model(structure = mod1_struct, with_dropout = TRUE, dropout_rate = 0.001, with_l1 = TRUE, l1_rate = 0.01)

h <- m %>% fit(
      x_train,
      y_train,
      epochs = 300,
      batch_size = 512, 
      validation_data = list(x_val, y_val)
)

plot(h)

```

Ok, so maybe not much improvement. Let's try a range of l1s and see.


```{r, l1s, eval = TRUE, message = FALSE, warning = FALSE, fig.width = 13, cache = TRUE}
result_l1 <- data.frame(l1 = double(), epoch = integer(), value = double(), metric = character(), data = character() )

for (l1 in c(0.001, 0.01, 0.02, 0.05, 0.1 )){
  epochs <- 300
  m <- make_model(structure = mod1_struct, with_dropout = TRUE, dropout_rate = 0.001, with_l1 = TRUE, l1_rate = l1)

  h <- m %>% fit(
      x_train,
      y_train,
      epochs = epochs,
      batch_size = 512, 
      validation_data = list(x_val, y_val)
  )
  df_h <- as.data.frame( h )
  df_h$l1 <- rep(l1, epochs) 
  result_l1 <- dplyr::bind_rows(result_l1, df_h)
  
  
}

ggplot(result_l1) + aes(epoch, value) + geom_point(aes(colour = data), alpha= 0.2) + geom_smooth(aes(colour = data)) + facet_grid(metric ~ l1, scales = "free")
```

So big l1s are bad, but the lowest here isn't having that much improvement. Not a big win with L1 regularization.


Quick check for L2.

```{r, l2s, eval = TRUE, message = FALSE, warning = FALSE, fig.width = 13, cache = TRUE}
result_l2 <- data.frame(l1 = double(), epoch = integer(), value = double(), metric = character(), data = character() )

for (l2 in c(0.001, 0.01, 0.02, 0.05, 0.1 )){
  epochs <- 300
  m <- make_model(structure = mod1_struct, with_dropout = TRUE, dropout_rate = 0.001, with_l2 = TRUE, l2_rate = l2)

  h <- m %>% fit(
      x_train,
      y_train,
      epochs = epochs,
      batch_size = 512, 
      validation_data = list(x_val, y_val)
  )
  df_h <- as.data.frame( h )
  df_h$l2 <- rep(l2, epochs) 
  result_l2 <- dplyr::bind_rows(result_l2, df_h)
  
  
}

ggplot(result_l2) + aes(epoch, value) + geom_point(aes(colour = data), alpha= 0.2) + geom_smooth(aes(colour = data)) + facet_grid(metric ~ l2, scales = "free")
```

Same. No advances. So with no other optimisation to perform for this sort of network I'll stick with the structure and dropout.


## Final Test

Having built a model, we can retrain and get final assessment by running on the test data.

```{r}
m <- make_model(structure = mod1_struct, with_dropout = TRUE, dropout_rate = 0.001)
  h <- m %>% fit(
      x_train,
      y_train,
      epochs = 300,
      batch_size = 512
  )

result <- m %>% evaluate(x_test, y_test)
result
```